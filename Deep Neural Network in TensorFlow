from tensorflow.examples.tutorials.mnist import input_data
mnist=input_data(".",one_hot=True,reshape=False)

#learing parameters
learning_rate=0.01
training_epoches=20
batch_size=128
display=1
n_input=784
n_classes=10

#hidden parameters
n_hidden_layer=256

#weights and biases
weights={
    'hidden_layer':tf.Variable(tf.random_normal([n_input,n_hidden_layer]))
    'out':tf.Variable(tf.random_normal([n_hidden_layer,n_classes]))
}
biases={
    'hidden_layer':tf.Variable(tf.random_normal([n_hidden_layer]))
    'out':tf.Variable(tf.random_normal([n_classes]))
}

#tf Graph input
x=tf.placeholder(float32,[None,28,28,1])
y=tf.placeholder(float32,[None,n_classes])
x_flat=tf.reshape(x,[-1,n_input])


#hidden layer with RELU activation 
layer_1=tf.add(tf.matmul(x_flat,weights['hidden_layer']),biases[hidden_layer])
layer_1=tf.nn.relu(layer_1)
#output layer with linear activation
logits=tf.add(tf.matmul(layer_1,weights['out']),biases['out'])

#define optimizer
cost=tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=y))
optimizer=tf.trian.GradientDescentOptimizer(learning_rate=learning_rate.minimize(cost))

#initializing the variables
init=tf.globa_variables_initializer()

#launch the graph
with tf.Session() as sess:
  sess.run(init)
  #training circle
  for epoch in range(training_epoches):
    total_batch=int(mnist.train.num_examples/batch_size)
    #loop all batches
    for i in range(total_batch):
      batch_x,batch_y=mnist.train.next_batch(batch_size)
      sess.run(optimizer,feed_dirct={x:batch_x,y:batch_y})
